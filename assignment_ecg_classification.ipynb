{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/uu-sml/wasp-assigninmen-af-classification/blob/main/assignment_ecg_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z00pmDU9qUai"
   },
   "source": [
    "# WASP Course: Artificial Intelligence and Machine Learning\n",
    "\n",
    "Lecturer: Dave Zachariah\n",
    "\n",
    "Assignment responsible: Philipp Pilar, Jingwei Hu, Paul HÃ¤usner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student and Group Information\n",
    "\n",
    "Fill this out for the submission of the assignment (you submit this notebook with your solution)\n",
    "\n",
    "- **Student names:** <font color='red'>Konstantinos Ioannis Sotiropoulos Pesiridis</font>\n",
    "\n",
    "- **Team ID:** <font color='red'>7N4T9Z3PQ5B6R8F2X1YW</font>\n",
    "\n",
    "Make sure that the team id is the same as the one with which you submit your model predictions (see coding task 7) such that we can check your performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_X8-WK_Htgr"
   },
   "source": [
    "---\n",
    "# Module 3 - Assignment Overview: ECG classification\n",
    "\n",
    "The [electrocardiogram (ECG)](https://www.mayoclinic.org/tests-procedures/ekg/about/pac-20384983) records the electrical signals in the heart. It is a common  test used to quickly detect heart problems and to monitor the heart's health. \n",
    "In this assignment you will implement and evaluate a model to classify whether the person has [atrial fibrillation (AF)](https://www.mayoclinic.org/diseases-conditions/atrial-fibrillation/symptoms-causes/syc-20350624.) or not based on measurements from the ECG exam. \n",
    "\n",
    "\n",
    "**Submission:** You submit the deliverables (see below) at https://canvas.kth.se/courses/48799/assignments\n",
    "\n",
    "**Due Date:** August 23, 2024.\n",
    "\n",
    "---\n",
    "## Basic Tasks\n",
    "Your task is to implement a classification model, train this model on training data, and evaluate its performance on validation data. We provide skeleton code for the implementation of a simple convolution neural network model.\n",
    "\n",
    "The steps required to implement this model are presented as numbered tasks below. In total there are seven (7) coding tasks and five (5) explanation tasks. \n",
    "\n",
    "## Competitive setting\n",
    "\n",
    "You have to compute the predictions for the test data (you do not have the labels for it) and submit your predictions to be evaluated to a leaderboard. These predictions will be scored and your submission will be ranked according to the F1 score and compared with your colleagues. In the end a winning team will be determined.\n",
    "\n",
    "### Deliverables\n",
    "There are two deliverables:\n",
    "1. You have to submit this Jupyter notebook on the course web-page (Canvas) together with your code and explanations (where asked for it) that describe your implementation and your experimental results. The notebook should run as a standalone in google colab.\n",
    "2. You have to have at least **three (3)** submissions (for instructions on how to submit, see coding task 7) where you try to improve the model architecture, the training procedure or the problem formulation. In the submission of this notebook you have to provide a short explanation of what changed between each submission and justify why you decided to make these changes.\n",
    "\n",
    "### Grading\n",
    "To pass the assignment, you must submit a complete and working implementation of a model and a well-motivated description and evaluation of it. Your model should reach an Area under the ROC curve (AUROC) on the test data of at least 0.97 and an Average Precision (AP) score of 0.95. Note that the leaderboard to is sorted by F1 score and not AUROC, hence you would want to balance all three metrics.\n",
    "\n",
    "### GPU Acceleration\n",
    "To be able to use the GPUs provided by colab in order to speed up your computations, you want to check that the `Hardware accelerator` is set to `GPU` under `Runtime > change runtime type`. Note that notebooks run by connecting to virtual machines that have maximum lifetimes that can be as much as 12 hours. Notebooks will also disconnect from VMs when left idle for too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# helper function\n",
    "def exists(path):\n",
    "    val = os.path.exists(path)\n",
    "    if val:\n",
    "        print(f'{path} already exits. Using cached. Delete it manually to recieve it again!')\n",
    "    return val\n",
    "\n",
    "# clone requirements.txt if not yet available\n",
    "if not exists('requirements.txt'):\n",
    "    !git clone https://gist.github.com/dgedon/8a7b91714568dc35d0527233e9ceada4.git req\n",
    "    !mv req/requirements.txt .\n",
    "    !yes | rm -r req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ_o0wKcm7WM"
   },
   "outputs": [],
   "source": [
    "# Install packages (python>=3.9 is required)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install scikit-learn for auroc\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:24:52.564049Z",
     "start_time": "2024-08-24T21:24:50.216285Z"
    },
    "id": "JkjZfvuKHd6q"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress matplotlib deprecation warnings\n",
    "import warnings\n",
    "from matplotlib import MatplotlibDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MatplotlibDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTfxCz1YIpGP"
   },
   "source": [
    "---\n",
    "## The data set\n",
    "\n",
    "The dataset is a subset of the [*CODE dataset*](https://scilifelab.figshare.com/articles/dataset/CODE_dataset/15169716): an anotated database of ECGs. The ECG exams were recorded in Brazil by the Telehealth Network of the state Minas Gerais between 2010 and 2016. The dataset and its usage for the development of deep learning methods was described in [\"Automatic diagnosis of the 12-lead ECG using a deep neural network\"](https://www.nature.com/articles/s41467-020-15432-4).\n",
    "The full dataset is available for research upon request.\n",
    "\n",
    "\n",
    "For the training dataset you have labels. \n",
    "For the test dataset you only have the ECG exams but no labels. Evaluation is done by submitting to the leaderboard.\n",
    "\n",
    "Download the dataset from the given dropbox link and unzip the folder containing the files. The downloaded files are in WFDB format (see [here](https://www.physionet.org/content/wfdb-python/3.4.1/) for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeNTb95FM6R1"
   },
   "outputs": [],
   "source": [
    "# 1. Download dataset\n",
    "if not exists('codesubset.tar.gz'):\n",
    "    !wget https://www.dropbox.com/s/9zkqa5y5jqakdil/codesubset.tar.gz?dl=0 -O codesubset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQR2EI49OVP0"
   },
   "outputs": [],
   "source": [
    "# 1. unzip the downloaded data set folder\n",
    "if not exists('codesubset'):\n",
    "    !tar -xf codesubset.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the extraced folder 'codesubset' contains\n",
    "1. subfolders with the ECG exam traces. These have to be further preprocessed which we do in the next steps.\n",
    "2. a csv file which contain the labels and other features for the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H8tsO-QABmw"
   },
   "source": [
    "\n",
    "### Preprocessing\n",
    "\n",
    "Run the cells below to  Clone the GitHub repository which we use for [data preprocessing](https://github.com/antonior92/ecg-preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yz7VW7nQEO-",
    "outputId": "9edf7844-03f0-4833-ecb6-2c53cda99f03"
   },
   "outputs": [],
   "source": [
    "# 2. clone the code files for data preprocessing\n",
    "if not exists('ecg-preprocessing'):\n",
    "    !git clone https://github.com/paulhausner/ecg-preprocessing.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlvD4bEtQUhc"
   },
   "source": [
    "Let us plot an ECG sample. We can plot ECGs using the `ecg_plot` library for example by using the following code snippet where `ecg_sample` is an array of size `(number of leads * sequence length)`. Now we can view an ECG before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 987
    },
    "id": "QOpX0vlEQVUU",
    "outputId": "4f57693a-b62e-4881-ab61-daace2bdeb31"
   },
   "outputs": [],
   "source": [
    "import ecg_plot\n",
    "runfile(\"ecg-preprocessing/read_ecg.py\")\n",
    "\n",
    "PATH_TO_WFDB = 'codesubset/train/TNMG624478'\n",
    "ecg_sample, sample_rate, _ = read_ecg(PATH_TO_WFDB)\n",
    "\n",
    "# ECG plot\n",
    "plt.figure()\n",
    "lead = ['I', 'II', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "ecg_plot.plot(ecg_sample, sample_rate=sample_rate, style='bw', row_height=8, lead_index=lead, columns=1, title='Sample ECG before pre-processing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_icbSxgDowE"
   },
   "source": [
    "\n",
    "The preprocessing consist of:\n",
    "- resampling all ECG traces to the sample sampling period (400 Hz). Option: ``--new_freq 400``\n",
    "- zero padding if necessary such that all ECG have the same number of samples (4096). Option: ``--new_len 4096``.\n",
    "- removing trends in the ECG signal. Option: ``--remove_baseline``\n",
    "- remove possible power line noise. Option: ``--powerline 60``\n",
    "\n",
    "You can run the script bellow to plot the same ECG after the preprocessing.  The script also use the  `ecg_plot` library (as you did above).  You can try also with different command line options to see how the preprocessing affects the signal that will be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "lDQfCECcDoGN",
    "outputId": "33526310-7605-4a9d-b055-97a0366ed6ba"
   },
   "outputs": [],
   "source": [
    "%run ecg-preprocessing/plot_from_ecg.py codesubset/train/TNMG624478 --new_freq 400 --new_len 4096 --remove_baseline --powerline 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Kh8RkYD8bE"
   },
   "source": [
    "Next we perform the preprocessing in all exams and convert them into one single h5 file (see [here](https://www.h5py.org/#:~:text=The%20h5py%20package%20is%20a,they%20were%20real%20NumPy%20arrays.) for details about the format). The resulting h5 files contains the traces as arrays with the shape `(number of traces * sequence length * number of leads)` where sequence length is 4096 and number of leads is 8. \n",
    "The files `train.h5` and `test.h5` will be saved inside the folder `codesubset/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyS0WXTzQU5c",
    "outputId": "d56a623b-48a0-4284-9711-d92fbec1a0f6"
   },
   "outputs": [],
   "source": [
    "# 3. Generate train\n",
    "if not exists('codesubset/train.h5'):\n",
    "    !python ecg-preprocessing/generate_h5.py --new_freq 400 --new_len 4096 --remove_baseline --powerline 60 codesubset/train/RECORDS.txt codesubset/train.h5\n",
    "# 3. Generate test\n",
    "if not exists('codesubset/test.h5'):\n",
    "    !python ecg-preprocessing/generate_h5.py --new_freq 400 --new_len 4096 --remove_baseline --powerline 60 codesubset/test/RECORDS.txt codesubset/test.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Task 1: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Analysis artifact 1: Accuracy of a random guessing classifier\n",
    "\n",
    "# Load labels\n",
    "csv = pd.read_csv(\"codesubset/train.csv\", delimiter=',')\n",
    "labels = csv[\"AF\"].values\n",
    "\n",
    "# Calculate number of positive/negative cases\n",
    "total_samples = len(labels)\n",
    "positive_samples = np.sum(labels)\n",
    "negative_samples = total_samples - positive_samples\n",
    "print(f\"Positive samples: {positive_samples}, Negative samples: {negative_samples}\")\n",
    "\n",
    "# Baseline comparison: always guessing the majority class\n",
    "majority_class_prediction = 1 if positive_samples > negative_samples else 0\n",
    "majority_accuracy = np.sum(majority_class_prediction == labels) / total_samples\n",
    "print(f\"Majority Classifier Accuracy: {majority_accuracy:.4f}\")\n",
    "\n",
    "# Define a random guessing classifier\n",
    "def random_guessing_classifier(n_samples, pos_prob):\n",
    "    return np.random.choice([0, 1], size=n_samples, p=[1 - pos_prob, pos_prob])\n",
    "\n",
    "# Run the random guessing classifier\n",
    "pos_prob = positive_samples / total_samples  # Probability of predicting AF\n",
    "predictions = random_guessing_classifier(total_samples, pos_prob)\n",
    "\n",
    "# Calculate the expected performance of a random guessing classifier\n",
    "expected_accuracy = (positive_samples / total_samples) ** 2 + (negative_samples / total_samples) ** 2\n",
    "print(f\"Expected Accuracy of a Random Guessing Classifier: {expected_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the performance of the random guessing classifier\n",
    "accuracy = np.sum(predictions == labels) / total_samples\n",
    "print(f\"Random Guessing Classifier Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ja8j1xOYJdqg",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "498aacf4-f018-44ee-cb38-aaccfa943177"
   },
   "outputs": [],
   "source": [
    "# Analysis artifact 2: An interactive classification game, uncomment last line in cell to play :)\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load the data\n",
    "csv = pd.read_csv(\"codesubset/train.csv\", delimiter=',')\n",
    "f = h5py.File(\"codesubset/train.h5\", \"r\")\n",
    "data = f[\"tracings\"]\n",
    "\n",
    "points = 0\n",
    "attempts = 0\n",
    "last_exam_id = -1\n",
    "\n",
    "# Initialize metrics\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(3)\n",
    "\n",
    "def play_game():\n",
    "    global points, attempts, last_exam_id, TP, FP, FN\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Randomly select a row\n",
    "        row_number = random.randint(0, len(csv) - 1)\n",
    "        af = csv.iloc[row_number][\"AF\"]\n",
    "        exam_id = csv.iloc[row_number][\"id_exam\"]\n",
    "        \n",
    "        # Plot the ECG data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ecg_plot.plot(data[row_number].T, sample_rate=400, row_height=8, lead_index=lead, columns=1, title=f\"Exam {exam_id}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Get user input for the guess\n",
    "        print(f\"Last exam id: {last_exam_id}\")\n",
    "        print(f\"Score: {points}/{attempts}\")\n",
    "        print(f\"Metrics: TP={TP}, FP={FP}, FN={FN}\")\n",
    "        guess = input(\"Enter your guess (0 for No AF, 1 for AF, or 2 to quit): \")\n",
    "        \n",
    "        # Check if the user wants to quit\n",
    "        if guess == \"2\":\n",
    "            print(\"Game ended by user.\")\n",
    "            print(f\"Final Score: {points}/{attempts}\")\n",
    "            print(f\"Final Metrics: TP={TP}, FP={FP}, FN={FN}\")\n",
    "            break\n",
    "        \n",
    "        # Convert input to integer and check the answer\n",
    "        try:\n",
    "            guess = int(guess)\n",
    "        except ValueError:\n",
    "            print(\"Invalid input, please enter a number (0, 1, or 2).\")\n",
    "            continue\n",
    "        \n",
    "        attempts += 1\n",
    "        \n",
    "        # Update metrics\n",
    "        if guess == af:\n",
    "            points += 1\n",
    "\n",
    "        if af == 1:\n",
    "            TP += 1\n",
    "        \n",
    "        if guess == 1 and af == 0:  # User predicts AF\n",
    "            FP += 1\n",
    "        elif guess == 0 and af == 1:\n",
    "            FN += 1\n",
    "\n",
    "        last_exam_id = exam_id\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# Start the game, uncomment the line below\n",
    "# play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu4tpfc3STcD",
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Explanation task 1: Data Analysis\n",
    "\n",
    "Main Findings from Data Analysis:\n",
    "- Class Imbalance: The dataset is heavily imbalanced, with 70% of samples being negative for Atrial Fibrillation (AF) and only 30% positive. A classifier that always predicts the majority class (negative) would achieve 70% accuracy.\n",
    "- Random Guessing: A random guessing classifier, which predicts based on the observed class distribution, would achieve an expected accuracy of about 58%, close to what was observed in practice (58.09%).\n",
    "- Interactive Classification Experiment: I developed an interactive game to manually classify ECGs as AF or non-AF, using two visual rules: irregular R-R intervals and indistinct P-waves. After educating myself for a couple of hours in yoututebe and over 100 ECGs, I achieved a precision of 56.25%, sensitivity of 58.7%, and an F1-score of 57.4%. Quite disappointing :(\n",
    "\n",
    "Preprocessing Steps and Their Importance:\n",
    "- Resampling to 400 Hz: Standardizes the sampling rate across all ECG traces.\n",
    "- Zero Padding to 4096 Samples: Aligns the length of all ECG signals.\n",
    "- Removing Baseline Trends: Eliminates slow-moving artifacts from the ECG signal, which do not represent the heartâs electrical activity.\n",
    "- Removing Power Line Noise: Filters out interference from electrical power sources, which can obscure critical features in the ECG signal, ensuring a cleaner input for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO_E8qGUJ2Db"
   },
   "source": [
    "---\n",
    "## Coding Task 2: My Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:25:01.375166Z",
     "start_time": "2024-08-24T21:25:01.366511Z"
    },
    "id": "1BHovxZZLvkd",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=16, stride=4, dropout_prob=0.8):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size // 2 - 1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Shortcut with Max Pooling followed by 1x1 Conv\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=stride, stride=stride),  # Downsample by a factor of stride (e.g., 4)\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)  # Match the channel dimension\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout1(out)  # Apply dropout after the first convolution and activation\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(self.bn2(out))\n",
    "        out = self.dropout2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AFibDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AFibDetector, self).__init__()\n",
    "        kernel_size = 16\n",
    "        self.initial_conv = nn.Conv1d(in_channels=8, out_channels=64, kernel_size=kernel_size)\n",
    "        self.initial_bn = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Adding 4 Residual Blocks:\n",
    "        # 1. Increase the number of filters by 64, every 2nd block\n",
    "        # 2. Subsample by a factor of 4 at every block\n",
    "        stride = 4\n",
    "        self.res_block1 = ResidualBlock(in_channels=64, out_channels=64, kernel_size=kernel_size, stride=stride)\n",
    "        self.res_block2 = ResidualBlock(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=stride)\n",
    "        self.res_block3 = ResidualBlock(in_channels=128, out_channels=128, kernel_size=kernel_size, stride=stride)\n",
    "        self.res_block4 = ResidualBlock(in_channels=128, out_channels=256, kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "        # Calculate the number of flat features after the residual blocks\n",
    "        dummy_input = torch.zeros(1, 8, 4096)\n",
    "        dummy_output = self.initial_conv(dummy_input)\n",
    "        dummy_output = F.relu(self.initial_bn(dummy_output))\n",
    "        dummy_output = self.res_block1(dummy_output)\n",
    "        dummy_output = self.res_block2(dummy_output)\n",
    "        dummy_output = self.res_block3(dummy_output)\n",
    "        dummy_output = self.res_block4(dummy_output)\n",
    "        flat_features = dummy_output.numel()\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(flat_features, 256)\n",
    "        self.output = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.initial_bn(self.initial_conv(x)))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7besXJ1Qjax",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Explanation Task 2: Final Model\n",
    "\n",
    "- **Lack of Prior Experience with CNNs**:\n",
    "This is my first time working with Convolutional Neural Networks (CNNs), so I relied heavily on existing literature to guide my model      design choices.\n",
    "\n",
    "- **Inspiration from Existing Research**: \n",
    "The architecture of my model a copy, to the best of my understanding, to the one proposed in the paper titled *\"Automatic diagnosis of the 12-lead ECG using a deep neural network\"*.\n",
    "\n",
    "- **Use of Residual Blocks**: \n",
    "I chose to implement a 4 Residual Block architecture, as detailed in the paper, to leverage the deep learning benefits of residual networks, such as better gradient flow and the ability to train deeper models. While this architecture is designed to classify ECGs into a variety of conditions, I used it with the understanding that it might be more powerful than necessary for binary classification.\n",
    "\n",
    "- **Justification for Overkill**:\n",
    "Despite being potentially overkill for a binary classification task, I opted for this architecture because it has been proven effective in the broader context of ECG classification. I prioritized using a tried-and-tested model over experimenting with simpler architectures due to my limited experience.\n",
    "I acknowledge that there may be simpler or more efficient models that could achieve similar performance for this specific task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeWVnhT1L72u"
   },
   "source": [
    "---\n",
    "## Train function\n",
    "\n",
    "The function `train(...)` is called to in every epoch to train the model. The function loads the training data, makes predictions, compares predictions with true labels in the loss function and adapting the model parameters using stochastic gradient descent.\n",
    "\n",
    "In the code cell below there is the basic structure to load data from the data loader and to log your loss. The arguments of the function are explained by the use in the `main(...)` function below.\n",
    "\n",
    "If you are unfamiliar with PyTorch training loops, then this official [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) might help (especially section \"4. Train your Network\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHcflxJ1Wprw"
   },
   "source": [
    "### Coding Task 3: Fill training loop\n",
    "\n",
    "Fill the code cell below such that the model is training when `train(...)` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMLCx9Cahr7f"
   },
   "outputs": [],
   "source": [
    "def train_loop(epoch, dataloader, model, optimizer, loss_function, device):\n",
    "    # model to training mode (important to correctly handle dropout or batchnorm layers)\n",
    "    model.train()\n",
    "    # allocation\n",
    "    total_loss = 0  # accumulated loss\n",
    "    n_entries = 0   # accumulated number of data points\n",
    "    # progress bar def\n",
    "    train_pbar = tqdm(dataloader, desc=\"Training Epoch {epoch:2d}\".format(epoch=epoch), leave=True)\n",
    "    # training loop\n",
    "    for traces, diagnoses in train_pbar:\n",
    "        # data to device (CPU or GPU if available)\n",
    "        traces, diagnoses = traces.to(device), diagnoses.to(device)\n",
    "\n",
    "        # 1. Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # 2. Forward pass: compute the model's predictions for the input batch\n",
    "        predictions = model(traces)\n",
    "        # 3. Compute loss: compare the model output with the true labels\n",
    "        loss = loss_function(predictions, diagnoses)\n",
    "        # 4. Backward pass: compute the gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # 5. Optimization step: update the model's parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update accumulated values\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        n_entries += len(traces)\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'loss': total_loss / n_entries})\n",
    "    train_pbar.close()\n",
    "    return total_loss / n_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQnstiLiWB1"
   },
   "source": [
    "---\n",
    "## Eval function\n",
    "\n",
    "The `eval(...)` function is similar to the `train(...)` function but is used to evaluate the model on validation data without adapting the model parameters. You can prohibit computing gradients by using a `with torch.no_grad():` statement.\n",
    "\n",
    "Currenlty only the loss is logged here. Additionally you have to collect all your predictions and the true values in order to compute more metrics such as AUROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22G-f_ooWunl"
   },
   "source": [
    "### Coding Task 4: Fill evaluation loop\n",
    "Fill the code cell below such we obtain model predictions to evaluate the validation loss and collect the predictoin in order to compute other validation metrics in the `main(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWOvM9e5ijqk"
   },
   "outputs": [],
   "source": [
    "def eval_loop(epoch, dataloader, model, loss_function, device):\n",
    "    # model to evaluation mode (important to correctly handle dropout or batchnorm layers)\n",
    "    model.eval()\n",
    "    # allocation\n",
    "    total_loss = 0  # accumulated loss\n",
    "    n_entries = 0   # accumulated number of data points\n",
    "    valid_probs = []  # accumulated predicted probabilities\n",
    "    valid_true = [] # accumulated true labels\n",
    "    \n",
    "    # progress bar def\n",
    "    eval_pbar = tqdm(dataloader, desc=\"Evaluation Epoch {epoch:2d}\".format(epoch=epoch), leave=True)\n",
    "    # evaluation loop\n",
    "    for traces_cpu, diagnoses_cpu in eval_pbar:\n",
    "        # data to device (CPU or GPU if available)\n",
    "        traces, diagnoses = traces_cpu.to(device), diagnoses_cpu.to(device)\n",
    "\n",
    "        # Disable gradient computation for efficiency and to prevent changes to the model\n",
    "        with torch.no_grad():\n",
    "            # Forward pass: compute the model's predictions for the input batch\n",
    "            predictions = model(traces)\n",
    "\n",
    "            # Compute loss: compare the model output with the true labels\n",
    "            loss = loss_function(predictions, diagnoses)\n",
    "\n",
    "            # Store probabilities and true labels for later analysis\n",
    "            valid_probs.append(predictions.cpu().numpy())\n",
    "            valid_true.append(diagnoses_cpu.numpy())  # Use CPU data directly to avoid unnecessary GPU-to-CPU transfer\n",
    "\n",
    "        # Update accumulated values\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        n_entries += len(traces)\n",
    "\n",
    "        # Update progress bar\n",
    "        eval_pbar.set_postfix({'loss': total_loss / n_entries})\n",
    "    eval_pbar.close()\n",
    "    return total_loss / n_entries, np.vstack(valid_probs), np.vstack(valid_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtBEPHo7jEZP"
   },
   "source": [
    "---\n",
    "## Run Training\n",
    "\n",
    "In the code cell below there are some initial (non-optimal!) training hyperparameters. Further, we combine everything from above into training code. That means that we build the dataloaders, define the model/loss/optimizer and then train/validate the model over multiple epochs. Here, we save the model with the lowest validation loss as the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Eh5IsvQWy0y"
   },
   "source": [
    "### Coding Task 5: Combine everything to train/validate the model\n",
    "\n",
    "The following tasks are necessary in the code below\n",
    "- split the data into training and validation data\n",
    "- define the loss function\n",
    "- decide and implement validation metric(s) to evaluate and compare the model on\n",
    "\n",
    "Optional task:\n",
    "- include learning rate scheduler\n",
    "- take specific care about possible data inbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOz48rnxdGfp"
   },
   "source": [
    "### Coding Task 6: Run your model and adapt hyperparameters\n",
    "\n",
    "After you combined everything in task 5, now you run the code to evaluate the model. Based on the resulting validation metrics you tune\n",
    "- the training hyperparameters\n",
    "- the model architecture\n",
    "- the model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjGcqXpOdSbA"
   },
   "source": [
    "### Explanation Task 3: Hyperparameter\n",
    "\n",
    "- **Focus on Key Hyperparameters**:\n",
    "I concentrated on tuning the learning rate and batch size.\n",
    "\n",
    "- **Hyperparameter Grid Search**:\n",
    "I systematically tested the following combinations:\n",
    "    - **Batch Sizes**: `[8, 16, 32, 64, 128]`\n",
    "    - **Learning Rates**: `[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]`\n",
    "\n",
    "- **Results Documentation**:\n",
    "  - I recorded the results for each combination in terms of training/validation losses, AUROC, accuracy, and F1 scores over training epochs.\n",
    "  - The detailed statistics and plots are available in my GitHub repository: [GitHub Repo](https://github.com/kromancer/wasp-assignmen-af-classification), with files named as `\"lr_{learning_rate}_bs_{batch_size}.pdf\"`.\n",
    "\n",
    "- **Optimal Hyperparameters**:\n",
    "The best F1 score was achieved with a learning rate of `1e-4` and a batch size of `8`.\n",
    "\n",
    "- **Training Duration**:\n",
    "I used 80 epochs for training. After monitoring the metrics across different epoch counts, I observed that further training beyond 80 epochs did not lead to any noticeable improvement, indicating that the model had likely converged.\n",
    "\n",
    "- **Motivation**:\n",
    "The choice of these hyperparameters was driven by empirical evidence from my grid search, focusing on achieving the best possible F1 score while ensuring the model's stability and generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VNfdwn8IqQQ"
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# choose hyperparameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-1  \n",
    "num_epochs = 80\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQvRyaQcyvcM",
    "outputId": "6f0a25f4-da7d-44ef-ac3b-34ac3087db09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tqdm.write(\"Use device: {device:}\\n\".format(device=device))\n",
    "\n",
    "# =============== Build data loaders ======================================#\n",
    "tqdm.write(\"Building data loaders...\")\n",
    "\n",
    "path_to_h5_train, path_to_csv_train, path_to_records = 'codesubset/train.h5', 'codesubset/train.csv', 'codesubset/train/RECORDS.txt'\n",
    "\n",
    "# load traces\n",
    "traces = torch.tensor(h5py.File(path_to_h5_train, 'r')['tracings'][()], dtype=torch.float32).transpose(1,2)\n",
    "\n",
    "# load labels\n",
    "ids_traces = [int(x.split('TNMG')[1]) for x in list(pd.read_csv(path_to_records, header=None)[0])] # Get order of ids in traces\n",
    "df = pd.read_csv(path_to_csv_train)\n",
    "df.set_index('id_exam', inplace=True)\n",
    "df = df.reindex(ids_traces) # make sure the order is the same\n",
    "labels = torch.tensor(np.array(df['AF']), dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "# load dataset\n",
    "dataset = TensorDataset(traces, labels)\n",
    "len_dataset = len(dataset)\n",
    "n_classes = len(torch.unique(labels))\n",
    "\n",
    "# split data \n",
    "split_ratio = 0.95\n",
    "split_train = int(len_dataset * split_ratio)\n",
    "split_valid = len_dataset - split_train\n",
    "dataset_train, dataset_valid = random_split(dataset, [split_train, split_valid])\n",
    "\n",
    "# build data loaders\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)                         \n",
    "tqdm.write(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMOKrUn9jfca",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "# =============== Define model ============================================#\n",
    "tqdm.write(\"Define model...\")\n",
    "model = AFibDetector()\n",
    "model.to(device=device)\n",
    "tqdm.write(\"Done!\\n\")\n",
    "\n",
    "# =============== Define loss function ====================================#\n",
    "# Since our dataset is imbalanced 7-3 in favor of negatives, let's use a pos weight set to their ratio 7/3\n",
    "pos_weight = torch.tensor([7/3]).to(device)\n",
    "loss_function = nn.BCELoss(pos_weight)\n",
    "\n",
    "# =============== Define optimizer ========================================#\n",
    "tqdm.write(\"Define optimiser...\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "tqdm.write(\"Done!\\n\")\n",
    "\n",
    "# =============== Define lr scheduler =====================================#\n",
    "lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# =============== Train model =============================================#\n",
    "tqdm.write(\"Training...\")\n",
    "best_loss = np.Inf\n",
    "\n",
    "train_loss_all, valid_loss_all = [], []\n",
    "auroc_all, accuracy_all, f1_all = [], [], []\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in trange(1, num_epochs + 1):\n",
    "    # training loop\n",
    "    train_loss = train_loop(epoch, train_dataloader, model, optimizer, loss_function, device)\n",
    "    # validation loop\n",
    "    valid_loss, y_pred, y_true = eval_loop(epoch, valid_dataloader, model, loss_function, device)\n",
    "\n",
    "    # collect losses\n",
    "    train_loss_all.append(train_loss)\n",
    "    valid_loss_all.append(valid_loss)\n",
    "\n",
    "    # Use the raw output as it already contains probabilities\n",
    "    y_pred = torch.tensor(y_pred).numpy()\n",
    "    auroc = roc_auc_score(y_true, y_pred)\n",
    "    auroc_all.append(auroc)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_labels = (y_pred >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_true, y_pred_labels)\n",
    "    accuracy_all.append(accuracy)\n",
    "    f1 = f1_score(y_true, y_pred_labels)\n",
    "    f1_all.append(f1)\n",
    "\n",
    "    # save best model: here we save the model only for the lowest validation loss\n",
    "    if valid_loss < best_loss:\n",
    "        # Save model parameters\n",
    "        torch.save({'model': model.state_dict()}, 'model.pth') \n",
    "        # Update best validation loss\n",
    "        best_loss = valid_loss\n",
    "        # statement\n",
    "        model_save_state = \"Best model -> saved\"\n",
    "    else:\n",
    "        model_save_state = \"\"\n",
    "\n",
    "    # Print message\n",
    "    tqdm.write('Epoch {epoch:2d}: \\t'\n",
    "                'Train Loss {train_loss:.6f} \\t'\n",
    "                'Valid Loss {valid_loss:.6f} \\t'\n",
    "                '{model_save}'\n",
    "                .format(epoch=epoch,\n",
    "                        train_loss=train_loss,\n",
    "                        valid_loss=valid_loss,\n",
    "                        model_save=model_save_state)\n",
    "                    )\n",
    "\n",
    "    # Update learning rate with lr-scheduler\n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step(valid_loss)\n",
    "\n",
    "\n",
    "# Calculate the best points\n",
    "best_valid_loss_idx = valid_loss_all.index(min(valid_loss_all))\n",
    "best_auroc_idx = auroc_all.index(max(auroc_all))\n",
    "best_accuracy_idx = accuracy_all.index(max(accuracy_all))\n",
    "best_f1_idx = f1_all.index(max(f1_all))\n",
    "\n",
    "# Plot Losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_loss_all, label='Train Loss')\n",
    "plt.plot(valid_loss_all, label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.annotate(f'{valid_loss_all[best_valid_loss_idx]:.6f}',\n",
    "             xy=(best_valid_loss_idx, valid_loss_all[best_valid_loss_idx]),\n",
    "             xytext=(best_valid_loss_idx, valid_loss_all[best_valid_loss_idx]),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "# Plot AUROC\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(auroc_all, label='Validation AUROC')\n",
    "plt.title('AUROC over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUROC')\n",
    "plt.legend()\n",
    "plt.annotate(f'{auroc_all[best_auroc_idx]:.4f}',\n",
    "             xy=(best_auroc_idx, auroc_all[best_auroc_idx]),\n",
    "             xytext=(best_auroc_idx, auroc_all[best_auroc_idx]),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(accuracy_all, label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.annotate(f'{accuracy_all[best_accuracy_idx]:.4f}',\n",
    "             xy=(best_accuracy_idx, accuracy_all[best_accuracy_idx]),\n",
    "             xytext=(best_accuracy_idx, accuracy_all[best_accuracy_idx]),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(f1_all, label='Validation F1 Score')\n",
    "plt.title('F1 Score over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.annotate(f'{f1_all[best_f1_idx]:.4f}',\n",
    "             xy=(best_f1_idx, f1_all[best_f1_idx]),\n",
    "             xytext=(best_f1_idx, f1_all[best_f1_idx]),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"validation.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ouhd4vgt4jlS"
   },
   "source": [
    "---\n",
    "## Model Testing\n",
    "\n",
    "Since we saved our best model, we can now load the trained model and make predictions on the test data set. We save the predictions in a csv file which will be uploaded as part of the deliverables. Note that we take a `Sigmoid()` function on the model prediction in order to obtain soft predictions (probabilities) instead of hard predictions (0s or 1s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXJRnGTpW7Qv"
   },
   "source": [
    "### Coding Task 7: Make prediction for test data\n",
    "\n",
    "Here you do not really need to code but you have to:\n",
    "- replace the baseline model with your model. If you do not use colab then change the path to the model location to load the trained model)\n",
    "- run the script. The predictions are saved in the variable `soft_pred`.\n",
    "- upload your predictions to the leaderboard online (see instruction details below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:25:27.927759Z",
     "start_time": "2024-08-24T21:25:26.784811Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# build the dataloader once and re-use when running the cell below possibly multiple times.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# =============== Build data loaders ==========================================#\n",
    "tqdm.write(\"Building data loaders...\")\n",
    "path_to_h5_test = 'codesubset/test.h5'\n",
    "traces = torch.tensor(h5py.File(path_to_h5_test, 'r')['tracings'][()], dtype=torch.float32).transpose(1,2)\n",
    "dataset = TensorDataset(traces)\n",
    "len_dataset = len(dataset)\n",
    "# build data loaders\n",
    "test_dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "tqdm.write(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-24T21:25:29.455541Z"
    },
    "id": "0-m4CYeW_hvO",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# =============== Define model ================================================#\n",
    "tqdm.write(\"Define model...\")\n",
    "model = AFibDetector()\n",
    "\n",
    "# load stored model parameters\n",
    "ckpt = torch.load('eclair.pth', map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "# put model on device\n",
    "model.to(device=device)\n",
    "tqdm.write(\"Done!\\n\")\n",
    "\n",
    "# =============== Evaluate model ==============================================#\n",
    "model.eval()\n",
    "# allocation\n",
    "test_pred = torch.zeros(len_dataset,1)\n",
    "# progress bar def\n",
    "test_pbar = tqdm(test_dataloader, desc=\"Testing\")\n",
    "# evaluation loop\n",
    "end=0\n",
    "for traces in test_pbar:\n",
    "    # data to device\n",
    "    traces = traces[0].to(device)\n",
    "    start = end\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        model_output = model(traces)\n",
    "\n",
    "        # store output\n",
    "        end = min(start + len(model_output), test_pred.shape[0])\n",
    "        test_pred[start:end] = torch.nn.Sigmoid()(model_output).detach().cpu()\n",
    "\n",
    "test_pbar.close()\n",
    "\n",
    "# =============== Save predictions ============================================#\n",
    "soft_pred = np.stack((1-test_pred.numpy(), test_pred.numpy()),axis=1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMxVJxHsgGdT"
   },
   "source": [
    "To upload your predictions to the leaderboard, use the following code. There are the following steps to follow:\n",
    "1. Download the GitHub repository for the leaderboard submission system.\n",
    "2. Register your team with a **team id** and **password**. The password ensures that only your team can upload to your team id. Do only run the registration once.\n",
    "3. Upload you predictions as a new submission. There are some things to obey here:\n",
    "    - For each submission you have to attach a note for you to keep track of the submission in the leaderboard and for us to know which submission you refer to in your explanation. Choose something meaningful such as \"submission A\" or \"model B\".\n",
    "    - You can only get one prediction evaluated per day and you get the score the following day. If you do multiple submissions on the same day, the initial submission will be overwritten and thus only the final submission will be evaluated.\n",
    "    - Only a maximum of ***FIVE*** submissions will be evaluated. So make them count! (If you update an submission before it is evaluated it doesn't count)\n",
    "    - The evaluation score is published with you team_id and note at http://hyperion.it.uu.se:5050/leaderboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BPWcP20RHwu",
    "outputId": "9effbfc3-14fe-440c-a0e5-d64566754784"
   },
   "outputs": [],
   "source": [
    "# 1. Download repository for leaderboard submission system\n",
    "if not exists('leaderboard'):\n",
    "    !git clone https://gist.github.com/3ff6c4c867331c0bf334301842d753c7.git leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRaCnCLsgHPg"
   },
   "outputs": [],
   "source": [
    "# 2. Registration of your team\n",
    "host = \"http://hyperion.it.uu.se:5050/\"\n",
    "runfile(\"leaderboard/leaderboard_helpers.py\")\n",
    "\n",
    "\"\"\"\n",
    "TASK: Decide for a team_id (max 20 chars) and password. \n",
    "Do not change this after you have registered your team\n",
    "\"\"\"\n",
    "team_id = '' #Fill in a string\n",
    "password = '' #Fill in a string\n",
    "\n",
    "# run the registration\n",
    "r = register_team(team_id, password)\n",
    "if (r.status_code == 201):\n",
    "    print(\"Team registered successfully! Good luck\")\n",
    "elif not (r.status_code == 200):\n",
    "    raise Exception(\"You can not change your password once created. If you need help, please contact the teachers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I03QEBiegF7_",
    "outputId": "778464c4-f0c4-41b9-a40a-c0b1cc734c0f"
   },
   "outputs": [],
   "source": [
    "# 3. Upload the prediction as submission\n",
    "\n",
    "# Write a note about the training procedure so you can identify it in the leaderboard. e.g. 5 epochs, or First  (Max 20 characters)\n",
    "\"\"\"\n",
    "TASK: Add a note for you submission\n",
    "\"\"\"\n",
    "note = 'dandruff'\n",
    "team_id = '7N4T9Z3PQ5B6R8F2X1YW'\n",
    "password = ''\n",
    "\n",
    "host = \"http://hyperion.it.uu.se:5050/\"\n",
    "runfile(\"leaderboard/leaderboard_helpers.py\")\n",
    "\n",
    "# Submit the predictions to the leaderboard. Note, this also saves your submissions in your colab folder\n",
    "r = submit(team_id, password, soft_pred.tolist(), note)\n",
    "if r.status_code == 201:\n",
    "    print(\"Submission successful!\")\n",
    "elif r.status_code == 200:\n",
    "    print(\"Submission updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTKe9X-zTt7f"
   },
   "source": [
    "### Explanation Task 4: Submissions\n",
    "\n",
    "1. **Submission \"chair\"**:\n",
    "   - **Model**: Used the final model architecture with default hyperparameters.\n",
    "   - **Bug in Training**: Encountered an issue where I used `BCEWithLogitsLoss`, which re-applied the sigmoid function, despite the model's output already including a sigmoid. The correct loss function should have been `BCELoss`.\n",
    "   - **Result**: The incorrect loss function led to suboptimal training, impacting accuracy and F1 scores.\n",
    "2. **Submission \"dandruff\"**:\n",
    "   - **Bug Fix**: Corrected the loss function by switching to `BCELoss` to match the sigmoid activation in the model's output layer.\n",
    "   - **Handling Imbalance**: Noted the low accuracy and F1 score in the previous submission and addressed class imbalance by implementing a weighted loss function, increasing the importance of the minority class (positive cases).\n",
    "   - **Hyperparameters**: Chose a batch size of 128 and an initial learning rate of 1e-4. These values were selected after some informal experimentation but without a systematic approach.\n",
    "3. **Submission \"eclair\"**:\n",
    "   - **Systematic Hyperparameter Tuning**: Adopted a more structured approach to hyperparameter selection. Specifically, I tested combinations of learning rates (`[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]`) and batch sizes (`[8, 16, 32, 64, 128]`) to find the optimal settings.\n",
    "   - **Final Choice**: The selected hyperparameters, lr=0.0001 and bs=8, were based on the combination that provided the best F1 score.\n",
    "\n",
    "Team id: **<font color='red'>7N4T9Z3PQ5B6R8F2X1YW</font>**\n",
    "\n",
    "| Submission note | Accuracy | F1    | AUC   | AP    | Submission description                       |\n",
    "|-----------------|----------|-------|-------|-------|----------------------------------------------|\n",
    "| chair           | 0.5      | 0.667 | 0.964 | 0.948 | Final model, bug during training             |\n",
    "| dandruff        | 0.5      | 0.667 | 0.987 | 0.987 | Fixed bug, no hyper tuning, weighted loss fun|\n",
    "| eclair          | 0.5      | 0.667 | 0.985 | 0.980 | Play with lr and bs, chose lr=1e-4, bs=8     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw-GKZ4fVDhD"
   },
   "source": [
    "### Explanation Task 5: Reflection on Metrics\n",
    "\n",
    "- **AUC (Area Under the ROC Curve)**:\n",
    "  - **What it Measures**: AUC measures the ability of the model to distinguish between the positives and negatives across clasification thresholds. It summarizes the trade-off between the true positive rate (sensitivity) and the false positive rate (1-specificity).\n",
    "  - **Why It's Important**: A high AUC indicates that the model is good at ranking positive examples higher than negative ones, regardless of the specific threshold used for classification. This is useful in situations where the decision threshold may vary depending on the application or when the class distribution is imbalanced.\n",
    "\n",
    "- **AP (Average Precision)**:\n",
    "  - **What it Measures**: AP summarizes the precision-recall curve, which focuses on the trade-off between precision and recall for all possible thresholds. It is particularly useful when dealing with imbalanced datasets, as it gives more insight into how well the model handles the minority class.\n",
    "  - **Why It's Important**: AP is crucial when the positive class is rare, as it evaluates how well the model identifies positives while minimizing false positives. This is especially relevant in applications like medical diagnostics or fraud detection.\n",
    "\n",
    "- **F1 Score**:\n",
    "  - **What it Measures**: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both aspects. It is particularly useful when the costs of false positives and false negatives are different or when you want to avoid extreme values in either precision or recall.\n",
    "  - **Why It's Important**: F1 score is essential when you need a balance between precision and recall, and neither should be significantly worse than the other. This metric is critical for tasks where both types of errors (false positives and false negatives) have serious consequences.\n",
    "\n",
    "#### Why Not to Focus on Just One Metric\n",
    "\n",
    "- **Focusing Only on AUC**:\n",
    "  - **What Can Happen**: If you only focus on AUC, your model might perform well in ranking instances but might not be well-calibrated for a specific decision threshold. This could lead to suboptimal precision or recall when you actually apply the model to make decisions. For example, a high AUC might coincide with a low F1 score if the model struggles with precision or recall at the chosen threshold (as in my first submission \"chair\").\n",
    "  - **Real-World Impact**: In practical scenarios, focusing solely on AUC could result in a model that ranks predictions well but fails to accurately classify instances in a way that meets the specific needs of the application, especially if the positive class is rare.\n",
    "\n",
    "- **Balancing Metrics**:\n",
    "  - **Why It's Necessary**: Each metric offers a different perspective on model performance. AUC provides insight into ranking ability, AP focuses on handling the minority class, and F1 score ensures a balance between precision and recall. Focusing on all three metrics ensures that the model is not only good at ranking predictions but also makes reliable classifications that are suitable for the specific context of the problem.\n",
    "  - **Avoiding Pitfalls**: By considering all these metrics together, you avoid the pitfall of over-optimizing for one aspect of performance at the expense of others, leading to a more robust and well-rounded model.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "assignment_ecg_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
